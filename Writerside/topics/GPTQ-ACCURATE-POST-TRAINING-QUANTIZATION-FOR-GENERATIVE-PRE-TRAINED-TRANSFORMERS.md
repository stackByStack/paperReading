# GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS

## Abstract
- While there is emerging work on relieving this pressure via
  model compression, the applicability and performance of existing compression
  techniques is **limited by** the **scale and complexity** of GPT models.
- In this paper,
  we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on **approximate second-order information**, that is both **_highly-accurate and highly-efficient_**. 

## Introduction
- <a href="https://zhuanlan.zhihu.com/p/424631681">Byte Pair Encoding</a>

- <a href="https://zhuanlan.zhihu.com/p/569694801">Reading Note of A Survey of Quantization Methods for Efficient Neural Network Inference</a>

- 



























