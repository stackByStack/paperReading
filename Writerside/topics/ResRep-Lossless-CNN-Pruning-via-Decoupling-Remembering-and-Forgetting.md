# ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting

## Abstract

- We propose to re-parameterize a CNN into the remembering parts and forgetting parts, 
  - where the former learn to maintain the performance 
  - and the latter learn to prune
- Via training with regular **SGD** on the **former** but **a novel update rule with penalty gradients** on the latter, we realize **structured sparsity**.
- Then we equivalently **merge** the remembering and forgetting parts into the original architecture with narrower layers. 
- In this sense, ResRep can be viewed as a successful application of _Structural_ **Re-parameterization**

## Introduction

### sparsity

#### Global sparse momentum SGD for pruning very deep neural networks

![image_20240125_104700.png](image_20240125_104700.png)

To overcome the drawbacks of the two paradigms discussed above, 
- we intend to explicitly control the
eventual compression ratio **via end-to-end training** 
- by directly altering the **gradient flow** of momentum
SGD to **deviate the training direction** in order to achieve a **high compression ratio** as well as maintain
the accuracy

##### Approximation metrics
![image_20240125_105600.png](image_20240125_105600.png)

##### Rewritten SGD Rule
![image_20240125_110100.png](image_20240125_110100.png)

### Channel Pruning

- Our HRank is inspired by the discovery that
**the average rank** of multiple feature maps generated by a
single filter is always the **same**, regardless of the number of
image batches CNNs receive. 
- Based on HRank, we develop
a method that is mathematically formulated to prune filters
with **low-rank feature maps**. 
- The principle behind our pruning is that **low-rank feature maps** contain **less information**,
and thus pruned results can be easily reproduced.

### Quantization

#### Post training 4-bit quantization of convolutional networks for rapid-deployment
We use ACIQ for activation quantization and bias-correction for quantizing weights.
##### Analytical Clipping for Integer Quantization (ACIQ)

Assuming bit-width M, we would like to quantize the values in the tensor uniformly
to `2^M` discrete values.

![image_20240125_165800.png](image_20240125_165800.png)

![image_20240125_171000.png](image_20240125_171000.png)

![image_20240125_171200.png](image_20240125_171200.png)

![image_20240125_171400.png](image_20240125_171400.png)

##### Per-channel bit allocation

![image_20240125_174500.png](image_20240125_174500.png)


##### Bias-correction

![image_20240125_181000.png](image_20240125_181000.png)

![image_20240125_181100.png](image_20240125_181100.png)

#### Bi-real net

Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm.: **Combining 1-bit with original feature map shortcut to keep information**

#### FOCUSED QUANTIZATION FOR SPARSE CNN
- Shift quantization of weights, which quantizes weight values in a model
  to **powers-of-two** or zero, i.e. {0, ±1, ±2, ±4, . . .}, is of particular of interest, as multiplications
  in convolutions become **much-simpler bit-shift operation**s.
- **Fine-grained pruning**, however, is often in conflict with **quantization**, as pruning introduces various
  degrees of sparsity to different layers. 
- Linear quantization methods (integers) have **uniform**
  quantization levels and non-linear quantization (logarithmic, floating-point and shift) have **fine levels
  around zero** 
- but levels grow **further apart** as values get **larger in magnitude**. 
- Both linear and nonlinear
  quantization thus provide precision where **it is not actually required** in the case of a pruned CNN
- We address both issues by proposing a new
  approach to quantize parameters in CNNs which we call **focused quantization** (FQ) that **mixes** shift
  and re-centralized quantization methods.


![image_20240125_222800.png](image_20240125_222800.png){thumbnail="true"}
> For example, 5 bits and normalization information could be enough to represent original distribution.
> 






